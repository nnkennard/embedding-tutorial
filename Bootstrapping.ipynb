{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e044bb",
   "metadata": {},
   "source": [
    "In this section, we will look at different factors that can affect the stability of your word embeddings, and ways to mitigate these issues in order to make your observations robust.\n",
    "\n",
    "This notebook contains code to train various word2vec models using gensim, but we won't run them during the tutorial, in order to save time. Although you could run them yourself, I would not recommend using these settings for research or making design decisions about your experiments:\n",
    "\n",
    "* for research, make sure you train larger models\n",
    "* for design decisions, please consult the original papers, which show these effects over hundreds of runs.\n",
    "\n",
    "This notebook is just for us to get a sense of how different things can get. Go ahead and run it for fun if you like, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b3d29",
   "metadata": {},
   "source": [
    "`build_w2v_model` is the function we will use to build all our models. I tried to make it somewhat realistic, but that means that I can't build them all during the tutorial. So we'll just have to use results \"that I prepared earlier\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69539038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_w2v_model(sentences, seed=0, num_workers=8):\n",
    "  \"\"\"Build a basic word2vec model using gensim.\n",
    "  \n",
    "  Args:\n",
    "    sentences: corpus as list of lists of tokens\n",
    "    seed: random seed (gensim.models.Word2Vec parameter)\n",
    "    num_workers: number of threads to use (gensim.models.Word2Vec parameter)\n",
    "    \n",
    "  Returns:\n",
    "    gensim.models.Word2Vec model\n",
    "  \"\"\"\n",
    "  return Word2Vec(sentences=sentences, vector_size=10, window=5, min_count=10, workers=num_workers)\n",
    "  \n",
    "def embedding_corpus_from_docs(doc_list):\n",
    "  \"\"\"Build a corpus from training embeddings from a specific list of documents.\n",
    "    Args:\n",
    "      doc_list: list of documents, each in list of lists of tokens form\n",
    "      \n",
    "    Returns:\n",
    "      A list of lists of tokens comprising sentences from all the documents in order.\n",
    "  \"\"\"\n",
    "  return sum(doc_list, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc7e80",
   "metadata": {},
   "source": [
    "We will use a corpus of documents from /r/AskScience on Reddit, in the form of a dict:\n",
    "```\n",
    "{\n",
    "\"id_1\": [\n",
    "\t[\"This\", \"is\", \"a\", \"sentence\", \".\"],\n",
    "\t[\"This\", \"is\", \"another\", \"sentence\", \".\"],\n",
    "\t],\n",
    "\"id_2\": [\n",
    "\t[\"This\", \"is\", \"a\", \"third\", \"sentence .\"], \n",
    "\t[\"This\", \"is\", \"a\", \"fourth\", \"sentence\", \".\"],\n",
    "\t],\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "The various preprocessing steps, etc. are hidden in `util.py`. For now, we will just retrieve them and not pay attention to the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "askscience_docs = util.preprocess_askscience()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d6b30",
   "metadata": {},
   "source": [
    "First, we will compare two models trained with different random seeds. Setting `num_workers=1` makes the gensim training deterministic.\n",
    "\n",
    "\\* Note: you can also get 'random' results by using a higher `num_workers`, but this is due to thread scheduling and is not reproducible. The tradeoff might be worth it though, since you can train a lot faster, and by the end of this tutorial we will have ways of dealing with randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0eab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_corpus = embedding_corpus_from_docs(list(askscience_docs.values())[:1000])\n",
    "\n",
    "print(\"Starting\")\n",
    "w2v_1 = build_w2v_model(fixed_corpus, 23)\n",
    "print(\"Built first model\")\n",
    "w2v_2 = build_w2v_model(fixed_corpus, 96)\n",
    "print(\"Built second model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7afed8",
   "metadata": {},
   "source": [
    "We will compare two models using a list of salient words for /r/AskScience. I got these by doing some LDA shenanigans, but I am the least qualified person in the room to talk about LDA, so I will just gloss over that part.\n",
    "\n",
    "One way to quantify the changes is to look at the difference in cosines calculated with the two models. We plot the 380 pairwise cosines in a scatter plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56abd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASK_SCIENCE_WORDS = [\"bacteria\", \"plant\", \"species\", \"brain\",\n",
    "\"muscle\", \"sleep\", \"human\", \"galaxy\",]\n",
    "# \"space\", \"planet\", \"universe\", \"electricity\",\n",
    "# \"light\", \"magnetic\", \"field\", \"power\",\n",
    "# \"calorie\", \"chemical\", \"temperature\", \"pressure\"]\n",
    "\n",
    "def plot_cosine_changes(model_1, model_2, words):\n",
    "  cosine_pairs = []\n",
    "\n",
    "  for i, word1 in enumerate(words):\n",
    "      for word2 in words[i+1:]:\n",
    "          cosine_pairs.append((model_1.wv.distance(word1, word2), model_2.wv.distance(word1, word2) ))\n",
    "  x, y = zip(*cosine_pairs)\n",
    "  sns.scatterplot(x, y)\n",
    "  \n",
    "plot_cosine_changes(w2v_1, w2v_2, ASK_SCIENCE_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605acb2",
   "metadata": {},
   "source": [
    "Clearly, raw cosines are not very reliable -- while they haven't changed drastically, these are substantial changes considering both models were trained with the **exact same distribution of contexts**. \n",
    "\n",
    "A more realistic way to compare  we make qualitative judgments based on nearest neighbor lists. So instead, we should look at changes in nearest neighbor lists, and then try to quantify that change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_nearest_neighbors(model_1, model_2, words):\n",
    "  for word in words:\n",
    "    print(word)\n",
    "    nn_1 = [x[0] for x in model_1.wv.most_similar(positive=[word])]\n",
    "    nn_2 = [x[0] for x in model_2.wv.most_similar(positive=[word])]\n",
    "    print(nn_1)\n",
    "    print(nn_2)\n",
    "    \n",
    "compare_nearest_neighbors(w2v_1, w2v_2, ASK_SCIENCE_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ec0a6",
   "metadata": {},
   "source": [
    "To quantify the change in nearest neighbors, we can measure (1) the proportion of the set of nearest neighbors that has changed and (2) the change in rank of the words that haven't changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(l1, l2):\n",
    "  return len(set(l1).intersection(set(l2))) / len(set(l1).union(set(l2)))\n",
    "\n",
    "def quantify_nn_change(model_1, model_2, words):\n",
    "  rank_diffs = []\n",
    "\n",
    "  for word in words:\n",
    "    nn_1 = [x[0] for x in model_1.wv.most_similar(positive=[word])]\n",
    "    nn_2 = [x[0] for x in model_2.wv.most_similar(positive=[word])]\n",
    "    print(word, jaccard(nn_1, nn_2))\n",
    "    for nn in nn_1:\n",
    "      if nn not in nn_2:\n",
    "        continue\n",
    "      else:\n",
    "        rank_diffs.append(nn_1.index(nn) - nn_2.index(nn))\n",
    "  fig, ax = plt.subplots()\n",
    "  sns.histplot(rank_diffs)\n",
    "  ax.set_xlabel(\"Difference in nearest neighbor rank\")\n",
    "  \n",
    "quantify_nn_change(w2v_1, w2v_2, ASK_SCIENCE_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d20ef3",
   "metadata": {},
   "source": [
    "All of these changes are taking place for the exact same set of documents. However, changes to the set of documents can result in instability as well. Below, we train two models, each on 95% of the documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8501ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys1 = random.sample(sorted(askscience_docs.keys()), 5)#0.95 * len(askscience_docs))\n",
    "keys2 = random.sample(sorted(askscience_docs.keys()), 5)#0.95 * len(askscience_docs))\n",
    "\n",
    "w2v_1 = build_w2v_model( embedding_corpus_from_docs([askscience_docs[key] for key in keys1]))\n",
    "print(\"Built first model\")\n",
    "w2v_2 = build_w2v_model( embedding_corpus_from_docs([askscience_docs[key] for key in keys2]))\n",
    "print(\"Built second model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8244a",
   "metadata": {},
   "source": [
    "# Show results here\n",
    "\n",
    "This is really important because while a corpus of documents represents the mental model of the document creators, it represents only a *sample* of this mental model. We need to treat it in a statistically appropriate way. The fact that sampling can make such a huge difference indicates that we need to use bootstrapping (i.e. sampling with replacement)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd69a1",
   "metadata": {},
   "source": [
    "Note that it is also possible to sample at the sentence level. While this gives much more appealing bounds, it is likely to be misleading -- a word's contexts all over a document are likely to be similar, and we aren't really excluding a particular usage of the word by just excluding some of its occurences in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb650d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = random.sample(fixed_corpus, int(0.95 * len(fixed_corpus)))\n",
    "sentences2 = random.sample(fixed_corpus, int(0.95 * len(fixed_corpus)))\n",
    "\n",
    "w2v_1 = build_w2v_model(sentences1)\n",
    "print(\"Built first model\")\n",
    "w2v_2 = build_w2v_model(sentences2)\n",
    "print(\"Built second model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9e9c8",
   "metadata": {},
   "source": [
    "# Show results for sentence-level sampling here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab553ec4",
   "metadata": {},
   "source": [
    "The way to ensure robust results is to use bootstrapping, and report aggregated results over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_NUM = 20\n",
    "import tqdm\n",
    "\n",
    "bootstrap_models_1 = []\n",
    "for i in tqdm.tqdm(range(BOOTSTRAP_NUM)):\n",
    "  selected_keys = random.choices(sorted(askscience_docs.keys()), k=len(askscience_docs.keys()))[:10]\n",
    "  bootstrap_models_1.append(build_w2v_model(embedding_corpus_from_docs([askscience_docs[key] for key in selected_keys])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e3c9d",
   "metadata": {},
   "source": [
    "Let's compare this with another set of 20 bootstrapped models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf50ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_models_2 = []\n",
    "for i in tqdm.tqdm(range(BOOTSTRAP_NUM)):\n",
    "  selected_keys = random.choices(sorted(askscience_docs.keys()), k=len(askscience_docs.keys()))[:10]\n",
    "  bootstrap_models_2.append(build_w2v_model(embedding_corpus_from_docs([askscience_docs[key] for key in selected_keys])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6b419",
   "metadata": {},
   "source": [
    "We can compare the nearest neighbors by aggregating their ranks over each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_nearest_neighbors(words, model_list):\n",
    "  nearest_neighbor_rank_counter = collections.defaultdict(lambda:collections.defaultdict(list))\n",
    "  for model in model_list:\n",
    "    for word in words:\n",
    "      for i, nn in enumerate(model.wv.most_similar(positive=[word])):\n",
    "        nearest_neighbor_rank_counter[word][nn].append(i+1)\n",
    "\n",
    "  for k, v in nearest_neighbor_rank_counter.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    \n",
    "aggregate_nearest_neighbors(ASK_SCIENCE_WORDS, bootstrap_models_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_docs = [final_docs[k] for k in sorted(final_docs.keys())]\n",
    "\n",
    "sorted_keys = sorted(final_docs.keys())\n",
    "\n",
    "\n",
    "random.seed(131)\n",
    "bootstrapped_keylists = []\n",
    "for _ in range(20):\n",
    "    bootstrapped_keylists.append(random.choices(sorted_keys, k=len(sorted_keys)))\n",
    "    \n",
    "models = []   \n",
    "for i, key_list in enumerate(bootstrapped_keylists):\n",
    "    models.append(\n",
    "        build_w2v_model(\n",
    "            embedding_corpus_from_docs(\n",
    "                [final_docs[k] for k in key_list]), 4))\n",
    "    print(f'Built model {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaae01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# def get_lemma(word):\n",
    "#     lemma = wn.morphy(word)\n",
    "#     if lemma is None:\n",
    "#         return word\n",
    "#     else:\n",
    "#         return lemma\n",
    "    \n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# def get_lemma2(word):\n",
    "#     return WordNetLemmatizer().lemmatize(word)\n",
    "  \n",
    "# nltk.download('stopwords')\n",
    "# en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# def prepare_text_for_lda(text):\n",
    "#     tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 4]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_corpus = []\n",
    "# for doc_id, doc in askscience_docs.items():\n",
    "#   doc_tokens = sum(doc, [])\n",
    "#   lda_corpus.append([get_lemma(token) for token in doc_tokens if len(token) > 4 and token not in en_stop])\n",
    "\n",
    "# from gensim import corpora\n",
    "# dictionary = corpora.Dictionary(lda_corpus)\n",
    "# corpus = [dictionary.doc2bow(text) for text in lda_corpus]\n",
    "# import pickle\n",
    "# pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "# dictionary.save('dictionary.gensim')\n",
    "\n",
    "# import gensim\n",
    "# NUM_TOPICS = 20\n",
    "# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "# ldamodel.save('model5.gensim')\n",
    "# topics = ldamodel.print_topics(num_words=4)\n",
    "# for topic in topics:\n",
    "#     print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675204ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "blerp = \"\"\"(0, '0.091*\"light\" + 0.047*\"speed\" + 0.033*\"would\" + 0.022*\"move\"')\n",
    "(1, '0.017*\"article\" + 0.015*\"people\" + 0.013*\"scientific\" + 0.012*\"study\"')\n",
    "(2, '0.096*\"earth\" + 0.067*\"would\" + 0.053*\"planet\" + 0.043*\"space\"')\n",
    "(3, '0.056*\"cause\" + 0.034*\"effects\" + 0.026*\"effect\" + 0.020*\"damage\"')\n",
    "(4, '0.022*\"chemical\" + 0.019*\"reaction\" + 0.019*\"different\" + 0.015*\"material\"')\n",
    "(5, '0.040*\"animal\" + 0.037*\"humans\" + 0.031*\"human\" + 0.030*\"species\"')\n",
    "(6, '0.040*\"weight\" + 0.033*\"muscle\" + 0.032*\"drink\" + 0.024*\"calorie\"')\n",
    "(7, '0.032*\"image\" + 0.020*\"picture\" + 0.019*\"pattern\" + 0.017*\"computer\"')\n",
    "(8, '0.042*\"plant\" + 0.028*\"bacteria\" + 0.018*\"electricity\" + 0.017*\"hands\"')\n",
    "(9, '0.043*\"title\" + 0.041*\"ground\" + 0.022*\"cloud\" + 0.021*\"train\"')\n",
    "(10, '0.049*\"field\" + 0.026*\"magnetic\" + 0.017*\"project\" + 0.014*\"fields\"')\n",
    "(11, '0.173*\"would\" + 0.056*\"could\" + 0.038*\"possible\" + 0.020*\"years\"')\n",
    "(12, '0.023*\"question\" + 0.022*\"would\" + 0.022*\"thanks\" + 0.022*\"understand\"')\n",
    "(13, '0.129*\"water\" + 0.047*\"would\" + 0.045*\"temperature\" + 0.033*\"pressure\"')\n",
    "(14, '0.165*\"energy\" + 0.047*\"power\" + 0.031*\"charge\" + 0.030*\"increase\"')\n",
    "(15, '0.077*\"brain\" + 0.056*\"sound\" + 0.043*\"people\" + 0.037*\"person\"')\n",
    "(16, '0.039*\"question\" + 0.022*\"answer\" + 0.021*\"really\" + 0.021*\"could\"')\n",
    "(17, '0.032*\"galaxy\" + 0.019*\"shape\" + 0.018*\"point\" + 0.018*\"measure\"')\n",
    "(18, '0.049*\"universe\" + 0.035*\"matter\" + 0.026*\"black\" + 0.020*\"space\"')\n",
    "(19, '0.033*\"happen\" + 0.017*\"sleep\" + 0.016*\"notice\" + 0.014*\"something\"')\n",
    "\"\"\"\n",
    "\n",
    "print(blerp.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9b7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
