{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b211f6d5",
   "metadata": {},
   "source": [
    "Just imports below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b085ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gzip\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "import util\n",
    "import gensim.downloader as api\n",
    "import os\n",
    "\n",
    "from util import normalize\n",
    "# We will normalize all vectors because we mostly care about cosines and having all vectors\n",
    "# normalized reduces cosine to a dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae8e13",
   "metadata": {},
   "source": [
    "We are using a custom class for embedding models for two reasons:\n",
    "* to be able to write analogy calculation code from scratch so that we can examine it directly and add variations\n",
    "* to be able to use the same operations on vectors from gensim and vectors from other sources\n",
    "\n",
    "Referring to the different items in an analogy can lead to some confusion. In this code, we will refer to the four items in the analogy task as `a`, `a_`, `b`, `b_`. In the canonical example,\n",
    "\n",
    "`man:woman::king:queen` would be referred to with the names\n",
    "\n",
    "`a  :a_   ::b   :b_`   \n",
    "\n",
    "\n",
    "Another slightly inconvenient issue with working with word embeddings is that we deal with tokens, indices, and vectors at different times:\n",
    "* **tokens** when we collect data, and when we use evaluation sets\n",
    "* **vectors** when we are calculating cosines, analogies, offsets, etc.\n",
    "* **indices** when we want to quickly calculate nearest neighbors\n",
    "\n",
    "To make this easier, I've wrapped up code into the `EmbeddingModel` class, where the instance attributes `self.vectors`, `self.index_to_word`, and `self.word_to_index` help to switch between these ways of referring to the same concept.\n",
    "\n",
    "I will also refer to the token as `a`, `b`, etc; to indices as `idx_a`, `idx_b`, etc; and vectors as `vec_a`, `vec_b`, etc.\n",
    "\n",
    "\n",
    "I have implemented the variations of the analogy measurement as methods associated with `EmbeddingModel`, partially to make our code a bit neater.\n",
    "\n",
    "Note that this code uses `util.Analogy`, which is just a named tuple containing `a`, `a_`, `b`, and `b_s`, defined in `util.py`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(object):\n",
    "  def __init__(self, vectors, index_to_word):\n",
    "    # Since this is only going to be used for analogies, excluding words\n",
    "    # outside top 300k since that is the norm, and saving normalized vectors\n",
    "    # so that we can just do dot products to get cosines\n",
    "    \n",
    "    self.vectors = np.array([normalize(vec) for vec in vectors[:300000]])\n",
    "    self.index_to_word = index_to_word[:300000]\n",
    "    self.word_to_index = {w:i for i, w in enumerate(self.index_to_word)}\n",
    "\n",
    "  def get_vector_by_word(self, word):\n",
    "    \"\"\"Given a token, retrieve its embedding from this model.\"\"\"\n",
    "    return self.vectors[self.word_to_index[word]]\n",
    "  \n",
    "  def find_nearest_neighbor(self, main_vector, max_neighbors=None):\n",
    "    \"\"\"Find a vector's nearest neighbor(s) from this model based on cosine similarity.\n",
    "    \n",
    "    Using np.argpartition sometimes saves a little time (50%-ish faster)\n",
    "    \n",
    "    Args:\n",
    "      main_vector: The vector to search from. May be an embedding from this\n",
    "          model, or a calculated vector.\n",
    "      max_neighbors: if set, how many nearest neighbors to find. If\n",
    "          max_neighbors is None, return an ordering over all embeddings in the model.\n",
    "    \n",
    "    Returns:\n",
    "      indices of max_neighbors embeddings (or all embeddings), ordered in decreasing\n",
    "          order of cosine similarity with main_vector.\n",
    "\n",
    "    \"\"\"\n",
    "    sims = np.dot(self.vectors, -1 * normalize(main_vector))\n",
    "    if max_neighbors is None:\n",
    "      return np.argsort(sims).tolist()\n",
    "    else:\n",
    "      top_indices = np.argpartition(sims, max_neighbors)[:max_neighbors]\n",
    "      return sorted(top_indices, key=lambda i:sims[i])\n",
    "    \n",
    "  def _vectorise_analogy(self, analogy):\n",
    "    \"\"\"Retrieve the relevant embeddings from this model for an analogy question.\n",
    "    \n",
    "       Args:\n",
    "         analogy: a util.Analogy object containing an analogy question\n",
    "\n",
    "       Returns:\n",
    "         embeddings for a, a_, and b from the model.\n",
    "    \n",
    "    \"\"\"\n",
    "    return [self.get_vector_by_word(w) for w in [analogy.a, analogy.a_, analogy.b]]\n",
    "\n",
    "  def original_analogy(self, analogy):\n",
    "    \"\"\"Implements analogy as used in word2vec and GloVe originally.\n",
    "    \n",
    "       Calculates a_ + (b-a), finds the nearest neighbor maybe_b_ EXCLUDING {a_, b, a},\n",
    "       then checks to see if maybe_b_ is in the list of possible b_s.\n",
    "    \"\"\"\n",
    "    vec_a, vec_a_, vec_b = self._vectorise_analogy(analogy)\n",
    "    approx_vec_b_ = vec_a_ - vec_a + vec_b\n",
    "    excluded_indices =  [self.word_to_index[w] for w in [analogy.a, analogy.b, analogy.a_]]\n",
    "    for maybe_b_idx in self.find_nearest_neighbor(approx_vec_b_, max_neighbors=4):\n",
    "      if maybe_b_idx not in excluded_indices:\n",
    "        return self.index_to_word[maybe_b_idx] in analogy.b_s\n",
    "    assert False\n",
    "\n",
    "  def analogy_no_exclusion(self, analogy):\n",
    "    \"\"\"Implements analogy as spoken of colloquially.\n",
    "    \n",
    "      Calculates b + (a_ - a), finds the nearest neighbor maybe_b_, which\n",
    "      may be in the set {a_, b, a}, then checks to see if maybe_b_ is in\n",
    "      the list of possible b_s.\n",
    "    \"\"\"\n",
    "    vec_a, vec_a_, vec_b = self._vectorise_analogy(analogy)\n",
    "    approx_vec_b_ = vec_a_ - vec_a + vec_b\n",
    "    maybe_b_idx, = self.find_nearest_neighbor(approx_vec_b_, max_neighbors=1)\n",
    "    return self.index_to_word[maybe_b_idx] in analogy.b_s\n",
    "\n",
    "  def analogy_nearest_b(self, analogy):\n",
    "    \"\"\"Implements 'analogy' ignoring a and a_\n",
    "    \n",
    "      Returns the nearest neighbor of b. This works well in cases where the a-a_\n",
    "      and b-b_ distances are very small, e.g. for plurals.\n",
    "    \"\"\"\n",
    "    _, _, vec_b = self._vectorise_analogy(analogy)\n",
    "    maybe_b_idx = self.find_nearest_neighbor(vec_b, max_neighbors=2)[1]\n",
    "    return self.index_to_word[maybe_b_idx] in analogy.b_s\n",
    "  \n",
    "  def analogy_neighbor_check(self, analogy):\n",
    "    \"\"\"Returns the name of the vector b + (a_ - a)'s nearest neighbor (if named)\n",
    "    \n",
    "      Args:\n",
    "        analogy: a util.Analogy\n",
    "        \n",
    "      Returns:\n",
    "        a name from {a, a_, b, b_} if one of these vectors is the nearest neighbor\n",
    "        of b + (a_ - a). If not, returns 'other'.\n",
    "    \n",
    "    \"\"\"\n",
    "    vec_a, vec_a_, vec_b = self._vectorise_analogy(analogy)\n",
    "    approx_vec_b_ = vec_a_ - vec_a + vec_b\n",
    "    maybe_b_idx, = self.find_nearest_neighbor(approx_vec_b_, max_neighbors=1)\n",
    "    maybe_b_ = self.index_to_word[maybe_b_idx]\n",
    "    if maybe_b_ == analogy.a:\n",
    "      return \"a\"\n",
    "    elif maybe_b_ == analogy.a_:\n",
    "      return \"a_\"\n",
    "    elif maybe_b_ == analogy.b:\n",
    "      return \"b\"\n",
    "    elif maybe_b_ in analogy.b_s:\n",
    "      return \"b_\"\n",
    "    else:\n",
    "      return \"other\"\n",
    "    \n",
    "\n",
    "# This is just to more neatly cycle through the different analogy variations later on\n",
    "ANALOGY_FN_MAP = {\n",
    "    \"original\": lambda x: x.original_analogy,\n",
    "    \"no_exclusion\": lambda x: x.analogy_no_exclusion,\n",
    "    \"nearest_b\": lambda x: x.analogy_nearest_b,\n",
    "    #\"reversed_original\": lambda x: x.reversed_original_analogy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c7a14",
   "metadata": {},
   "source": [
    "Here, we are downloading prepared vectors from gensim, and saving them to disk. We won't run this cell more than once.\n",
    "\n",
    "Note: this is **a little bit sketchy software engineering-wise**, and it's more just in case the notebook crashes during the tutorial. If you use this code, you will have to ensure that the pickles saved to disk are deleted and re-created whenever you make a change to `EmbeddingModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import os\n",
    "\n",
    "def get_gensim_vectors(vector_name, model_save_name):\n",
    "  \"\"\"Retrieve pretrained vectors either from the API or from disk.\n",
    "  \n",
    "  Also, write EmbeddingModel object to disk because notebooks keep crashing.\n",
    "  \"\"\"\n",
    "  path = f'data/{model_save_name}.pkl'\n",
    "  if os.path.exists(path):\n",
    "    print(f\"Loading {model_save_name} from file\")\n",
    "    with open(path, 'rb') as f:\n",
    "      return pickle.load(f)\n",
    "  else:\n",
    "    print(f\"Loading {model_save_name} from gensim API\")\n",
    "    gensim_model = api.load(vector_name)\n",
    "    embedding_model = EmbeddingModel(gensim_model.vectors, gensim_model.index_to_key)\n",
    "    with open(f'data/{model_save_name}.pkl', 'wb') as f:\n",
    "      pickle.dump(embedding_model, f)\n",
    "    return embedding_model\n",
    "\n",
    "glove = get_gensim_vectors(\"glove-wiki-gigaword-100\", \"glove\")\n",
    "w2v =  get_gensim_vectors(\"word2vec-google-news-300\", \"w2v\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bbe810",
   "metadata": {},
   "source": [
    "Get analogy test sets and embeddings. Test sets are read in using functions in `util.py`.\n",
    "\n",
    "Notably, rather than having a single `b_`, we have a list of possible `b_`s. This is because in some categories, the BATS dataset has multiple possible b_ values, any of which can be considered correct.\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541dbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bats_analogy_dataset = util.read_bats_analogy_dataset()\n",
    "uncased_google_analogy_dataset = util.read_google_analogy_dataset(uncase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b96751",
   "metadata": {},
   "source": [
    "Below, we evaluate word2vec embeddings on some variations of the analogy task to shed some light on the analogy results. For brevity, we just run these on the first 200 examples in each category.\n",
    "\n",
    "The variations we use are:\n",
    "\n",
    "* \"Classic\": The classic analogy calculation, matching results in word2vec and GloVe papers, as well as the analogy calculation in the gensim library.\n",
    "* \"No exclusion\": If, unlike in the Classic setting, we allow `a`, `a_` and `b` to be possible answers.\n",
    "  * If the distance between `a` and `a_` is very small, `b` could be the retrieved answer.\n",
    "  * If `a` and `b` are very close, `a` and `a_` could be the answers.\n",
    "* \"Only b\": If we return the nearest neighbor `b` as the answer instead of doing any calculations.\n",
    "  * If the `a` - `a_` and `b` - `b_` distances tend to be very small, e.g. plurals, this variation could fare well.\n",
    "  \n",
    "We also collect results counting how often `a`, `a_` or `b` is the nearest neighbor of `b + (a_ - a)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec065ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_analogy_dataset(dataset, embedding_model, max_per_category=20000):\n",
    "  \"\"\"Evaluates an embedding model with an analogy dataset, using different variations.\n",
    "  \n",
    "  Args:\n",
    "  \n",
    "  Returns:\n",
    "  \n",
    "  \"\"\"\n",
    "  result_counter = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "  neighbor_count = collections.defaultdict(collections.Counter)\n",
    "  for category, analogies in tqdm.tqdm(list(dataset.items())):\n",
    "    skipped = 0\n",
    "    for analogy in analogies[:max_per_category]:\n",
    "      #print(analogy)\n",
    "      if not set(list(analogy[:3])+analogy[3]).issubset(embedding_model.index_to_word):\n",
    "        skipped += 1\n",
    "        continue\n",
    "      for eval_type, eval_fn in ANALOGY_FN_MAP.items():\n",
    "        result_counter[category][eval_type].append(eval_fn(embedding_model)(analogy))\n",
    "      neighbor_count[category][embedding_model.analogy_neighbor_check(analogy)] += 1\n",
    "    \n",
    "  return result_counter, neighbor_count\n",
    "#%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_analogy_results(analogy_results):\n",
    "  result_counter, neighbor_counter = analogy_results\n",
    "  \n",
    "  sorted_categories = sorted(neighbor_counter.keys())\n",
    "  \n",
    "  neighbor_array = np.zeros([len(neighbor_counter), 5])\n",
    "  for i, key in enumerate(sorted_categories):\n",
    "    for j, possible_answer in enumerate(\"a a_ b b_ other\".split()):\n",
    "      neighbor_array[i][j] =  neighbor_counter[key][possible_answer]\n",
    "      \n",
    "  analogy_style_array = np.zeros([len(result_counter), 3])\n",
    "  for i, key in enumerate(result_counter.keys()):\n",
    "    results = result_counter[key]\n",
    "    for j, style in enumerate(\"original no_exclusion nearest_b\".split()):\n",
    "      analogy_style_array[i][j] =  sum(results[style])/len(results[style])\n",
    "\n",
    "      \n",
    "  fig, ax = plt.subplots(1,2, figsize=(7,5), dpi=300)\n",
    "  sns.heatmap(neighbor_array, ax=ax[0])\n",
    "  ax[0].set_xticklabels(\"a a_ b b_ other\".split(), rotation=30, ha='center')\n",
    "  ax[0].set_yticklabels(sorted_categories, rotation=0)\n",
    "  \n",
    "  \n",
    "  sns.heatmap(analogy_style_array, ax=ax[1])\n",
    "  ax[1].set_xticklabels(\"original no_exclusion nearest_b\".split(), rotation=30, ha='center')\n",
    "  ax[1].set_yticklabels(sorted_categories, rotation=0)\n",
    "  \n",
    "  \n",
    "  plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "cased_google_analogy_dataset = util.read_google_analogy_dataset(uncase=False)\n",
    "for dataset in [cased_google_analogy_dataset, \n",
    "                #bats_analogy_dataset\n",
    "               ]:\n",
    "    plot_analogy_results(evaluate_analogy_dataset(dataset, w2v, 20))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6928fc2",
   "metadata": {},
   "source": [
    "In this section, we present a quick litmus test for semantic projection research.\n",
    "\n",
    "If we define a semantic dimension based on n pairs of antonyms, one way to check the robustness of this dimension is to hold out one pair of antonyms, and ensure that the 'positive' held-out antonym is closer to the positive end of the semantic dimension than the 'negative' held-out antonym. We plot these two cosines on the x and y axis below.\n",
    "\n",
    "We use word lists and vectors from [Geometry of Culture](??) in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe646f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_pairs = [('employer', 'employee'), ('employers', 'employees'), ('owner', 'worker'), \n",
    "                    ('proprietor', 'employee'), ('proprietors', 'employees'),\n",
    "                    ('capitalist', 'proletarian'), ('capitalists', 'proletariat'),\n",
    "                    ('manager', 'staff'), ('managers', 'staff'), ('director', 'employee'),\n",
    "                    ('directors', 'employees'), ('boss', 'worker'), ('bosses', 'workers'),\n",
    "                    ('foreman', 'laborer'), ('foremen', 'laborers'), ('supervisor', 'staff'),\n",
    "                    ('superintendent', 'staff')]\n",
    "employer_words, employee_words = zip(*employment_pairs)\n",
    "\n",
    "def get_coha_embeddings(year):\n",
    "    with open(f\"sgns/{year}-vocab.pkl\", 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vectors = np.load(f\"sgns/{year}-w.npy\")\n",
    "    return EmbeddingModel(vectors, vocab)\n",
    "  \n",
    "coha = get_coha_embeddings(1960)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa853fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "  return np.dot(normalize(a), normalize(b))\n",
    "\n",
    "\n",
    "def semantic_dimension_litmus_test(antonym_pairs, embeddings):\n",
    "  cosine_list = []\n",
    "  for held_out_pair in antonym_pairs:\n",
    "    # Get the rest of the pairs\n",
    "    retained_pairs = [pair for pair in antonym_pairs if not pair == held_out_pair]\n",
    "\n",
    "    # Separate the 'positive' and 'negative' antonyms, and sum each set into a positive and negative vector\n",
    "    retained_pos, retained_neg = zip(*retained_pairs)\n",
    "    pos_vector = sum([embeddings.get_vector_by_word(word) for word in retained_pos])\n",
    "    neg_vector = sum([embeddings.get_vector_by_word(word) for word in retained_neg])\n",
    "\n",
    "    # Build the semantic dimension vector by taking the difference of the positive and negative vectors\n",
    "    diff_vector = pos_vector - neg_vector\n",
    "\n",
    "\n",
    "    cosine_list.append((held_out_pair, \n",
    "                        cosine(diff_vector, embeddings.get_vector_by_word(held_out_pair[0])),\n",
    "                        cosine(diff_vector, embeddings.get_vector_by_word(held_out_pair[1])),\n",
    "                       ))\n",
    "\n",
    "  names, x, y = zip(*cosine_list)\n",
    "  fig, ax = plt.subplots()\n",
    "  sns.scatterplot(x=x, y=y, ax=ax)\n",
    "  ax.set_ylim(-1,1)\n",
    "  ax.set_xlim(-1,1)\n",
    "  ax.set_xlabel(\"Cosine of 'positive' held-out antonym with semantic dimension\")\n",
    "  ax.set_ylabel(\"Cosine of 'negative' held-out antonym\\n with semantic dimension\")\n",
    "  \n",
    "  for name, x, y in cosine_list:\n",
    "    if x < y:\n",
    "        ax.text(x=x-.05, y=y-.05, s=\"-\".join(name),ha='right')\n",
    "\n",
    "  plt.plot([-1,1], [-1, 1], linestyle=\"dashed\", lw=0.7, color=\"k\")\n",
    "  plt.tight_layout()\n",
    "    \n",
    "semantic_dimension_litmus_test(employment_pairs, coha)\n",
    "print()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
